{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9884157,"sourceType":"datasetVersion","datasetId":6069495}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Supervised Fine tuning (with label data)\n- Iam going to perform the supervised fine tuning (1000 samples dataset consist of instruction and responses ) by using the lama-3.2-1b model\n\n### Performing Quantization \n- Iam going to use Quantization to reduce the memory of 1b weights_biases of lama model from 32 to 4 bit to make it compatible to my pc with available resources\n\n### Performing lora based fine tuning \n- Iam going to utilize the lora technique to update specific task weights based on rank of matrix decomposition, rather than updating whole 1billion weights which makes incompatible with pc which has lower memory and gpus\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# working on\n!pip install huggingface_hub","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-13T09:53:19.485765Z","iopub.execute_input":"2024-11-13T09:53:19.486115Z","iopub.status.idle":"2024-11-13T09:53:32.015756Z","shell.execute_reply.started":"2024-11-13T09:53:19.486080Z","shell.execute_reply":"2024-11-13T09:53:32.014509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"# working on\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:10:01.944239Z","iopub.execute_input":"2024-11-13T10:10:01.945214Z","iopub.status.idle":"2024-11-13T10:10:01.969081Z","shell.execute_reply.started":"2024-11-13T10:10:01.945174Z","shell.execute_reply":"2024-11-13T10:10:01.967557Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d852cc49a4d24c85a2ad3777191ce1b5"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# working on\n!pip install accelerate peft bitsandbytes transformers trl","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-13T09:53:47.104145Z","iopub.execute_input":"2024-11-13T09:53:47.104541Z","iopub.status.idle":"2024-11-13T09:54:15.144798Z","shell.execute_reply.started":"2024-11-13T09:53:47.104502Z","shell.execute_reply":"2024-11-13T09:54:15.143837Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting trl\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, transformers, trl, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2 transformers-4.46.2 trl-0.12.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading the necessary functions from library","metadata":{}},{"cell_type":"code","source":"# working on\nimport os\nimport torch\nfrom datasets import load_dataset  # from the datasets library by Hugging Face, used for loading datasets for training.\nfrom transformers import (  \n    AutoModelForCausalLM, #  Automatically loads a pretrained causal language model suitable for generating text in a conversational setting. This is useful because it allows us to use pretrained weights and architectures from the transformers library\n    AutoTokenizer, # Automatically loads the tokenizer corresponding to a specific model, which is necessary for converting text into token IDs that the model can process.\n    BitsAndBytesConfig, # sed for optimizing model performance by setting configurations for 8-bit or 4-bit quantization\n    HfArgumentParser, # Used for parsing arguments in a structured way, making it easier to handle training and configuration arguments.\n    TrainingArguments, # Contains configurable options for training models (like batch size, learning rate, logging steps, etc.\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel # PeftModel specifically designed to handle Parameter-Efficient Fine-Tuning (PEFT). This wraps the base model and integrates the new parameters specified by LoRA without modifying the original model parameters\nfrom trl import SFTTrainer   # Supervised Data for Fine-Tuning: Supervised fine-tuning is performed on a labeled dataset where each example includes input text and the desired output. For example: If the model is fine-tuned for answering questions, the input could be a question, and the output is the answer. For a chatbot model, the input might be a user’s message, and the output is the expected chatbot response. In this scenario, the dataset's text examples serve as explicit guidance to teach the model specific patterns and responses aligned with the task requirements. Fine-Tuning Process: Fine-tuning involves training the model on this labeled data to minimize the difference between the model's predictions and the actual (supervised) target outputs provided in the dataset. During this training, the model adjusts its parameters to more accurately produce the correct output given similar inputs in the future.","metadata":{"execution":{"iopub.status.busy":"2024-11-13T09:54:15.146639Z","iopub.execute_input":"2024-11-13T09:54:15.146963Z","iopub.status.idle":"2024-11-13T09:54:35.289241Z","shell.execute_reply.started":"2024-11-13T09:54:15.146928Z","shell.execute_reply":"2024-11-13T09:54:35.288429Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **An label instruction-response pair dataset:**\nIt consists of 1,000 carefully curated instruction-response pairs. These pairs help language models learn how to respond to specific instructions, questions, or prompts in a conversational and informative manner.","metadata":{}},{"cell_type":"code","source":"# working on\nfrom datasets import load_dataset\n\n# Load the dataset from the Hugging Face Hub\ndataset = load_dataset(\"mlabonne/guanaco-llama2-1k\")\n\n# Inspect the data structure\nprint(dataset)\n\nprint(dataset[\"train\"][0:3])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-13T09:55:25.934236Z","iopub.execute_input":"2024-11-13T09:55:25.934702Z","iopub.status.idle":"2024-11-13T09:55:28.210393Z","shell.execute_reply.started":"2024-11-13T09:55:25.934659Z","shell.execute_reply":"2024-11-13T09:55:28.209504Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcdd9e1dcab42e49008722736674fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-9ad84bb9cf65a42f.parquet:   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93692fbed3cb41938650051ec2ce53d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98dd8da4bb9e4f2a9f79a9661220e47e"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1000\n    })\n})\n{'text': ['<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>', '<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\\n\\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \\n\\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \\n\\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>', '<s>[INST] Compose a professional email with the following points:\\n\\nMe chinese cook 10 years\\nGood good cook\\nPeople love my food\\nCan you hire me? [/INST] Thanks for the information. Unfortunately, your initial message does not contain enough details or structure to compose a compelling professional email to a potential employer. Here are some suggestions for how we can improve the message:\\n\\n1) Let\\'s first clarify the type of cooking position you are interested in. Are you looking for a job as a line cook, sous chef, executive chef, or something else? This will help determine how to emphasize relevant experience and skills.\\n\\n2) What specific experience do you have? For example, \"Over 10 years of experience preparing authentic Sichuan and Cantonese cuisine.\" Highlight any relevant skills, like knife skills, cooking techniques, menu development, etc. Mention the types of cuisine you specialize in.\\n\\n3) Have you worked in a professional kitchen before? If so, mention the name and type of restaurant (upscale, casual dining, etc.). Emphasize relevant responsibilities and accomplishments. If not, that\\'s fine too, just be clear that you\\'re looking to transition into a professional kitchen and highlight related skills and passion for cooking.\\n\\n4) Do you have any certifications or training? Note any culinary degrees, certificates, or other credentials. Even informal training can show your dedication to the craft.\\n\\n5) Are you looking for full-time, part-time, temporary, or contract work? Be specific about the type of opportunity you\\'re seeking.\\n\\nWith the answers to these questions, we can craft an email in a standard business letter format. The opening can express your interest in the position and summarize your relevant experience and qualifications. The body can go into more depth on your experience, skills, and goals. The closing can reiterate your interest and mention that you\\'d welcome the opportunity to further discuss the role.\\n\\nDoes this help? Let me know if you would like help crafting the full email. I\\'m happy to assist you in composing a professional and compelling message to potential employers. </s>']}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting Important parameters and configurations for Quantization, lora, Model training ","metadata":{}},{"cell_type":"code","source":"# working on\nmodel_name=\"meta-llama/Llama-3.2-1B\"\n\n# Fine-tuned model name\nnew_model = \"Llama-3.2-1B-chat-finetune\"\n\n\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"  # it consist of 1000 instructions-response paris , These pairs help language models learn how to respond to specific instructions, questions, or prompts in a conversational and informative manner\n\n\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# W'=W+ΔW  \n\n#where: W= The original weight matrix of the neural network layer. , The low-rank update matrix, which approximates the change needed to fine-tune the model. A A and B B: The low-rank matrices that form the approximation of the full-rank update by capturing key patterns in the adaptation needed. α α: The scaling factor that balances the influence of the original weights W W and the low-rank update Δ W ΔW. \n\n# ΔW=α⋅(A×B)\n\n\n# LoRA attention dimension\nlora_r = 16  # Specifies the rank of the low-rank matrix in LoRA layers. Higher values improve model capacity but increase computational needs.\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16  # A scaling factor in LoRA. It controls how much the LoRA weights contribute to the overall model. Higher values increase contribution.\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1  # The dropout rate applied to LoRA layers, set to 10% here. Dropout regularizes the model to reduce overfitting during fine-tuning.\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True   \n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"   # Specifies the data type for model computation. \"float16\" is half-precision \n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"  # Sets the quantization type. nf4 (normal float 4) is often used for better representation in low-bit settings.\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False  # Enables double quantization, which can further reduce memory but is disabled here.\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1  # The number of times the model will go through the entire dataset during training. Set to 1 for a single pass\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True   \nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 1\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 1\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 4  # Accumulates gradients over several steps before updating model weights, effectively increasing batch size without using extra memory.  it performs four forward passes, collecting gradients at each step, and then updates the model’s parameters once. This effectively increases the batch size to 4 (1 batch per step × × 4 steps), without using additional GPU memory.\n\n\n# Enable gradient checkpointing\ngradient_checkpointing = True  # Reduces memory by saving only the gradient checkpoints\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3 \n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4 \n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001  # Weight decay is a form of regularization that penalizes large weights, encouraging the model to find simpler representations. Here, 0.001 means the model’s weights (excluding biases and LayerNorm layers) are slightly penalized each step, discouraging overfitting.\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"   # The optimizer used for training. \"paged_adamw_32bit\" is a memory-efficient version of the AdamW optimizer, storing weights in 32-bit precision.\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"  #  This sets the learning rate schedule to \"cosine decay\". With cosine scheduling, the learning rate gradually decreases following a cosine curve, starting high and slowly approaching zero. This method is popular as it reduces the learning rate gradually and smoothly, often leading to better convergence.\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1   # Setting it to -1 means that the training will run for the number of epochs specified in num_train_epochs instead of a specific number of steps.\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03   # Warmup gradually increases the learning rate from 0 to the specified learning rate over a set number of steps. This ratio (0.03) means that the learning rate will increase linearly over the first 3% of the training steps before it reaches the main learning rate. This helps stabilize training at the beginning.\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = False\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT (Supervised Fine-Tuning) parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = 256  # Limits the maximum input sequence length to 256 tokens, trimming longer inputs., meaning only the first 256 tokens are processed. This length limit helps reduce memory usage and keeps processing time manageable, as longer sequences require more memory and compute resources\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU \ndevice_map = \"auto\"  # automatically assigns model layers to GPUs as needed.","metadata":{"execution":{"iopub.status.busy":"2024-11-13T09:55:28.212274Z","iopub.execute_input":"2024-11-13T09:55:28.212696Z","iopub.status.idle":"2024-11-13T09:55:28.550920Z","shell.execute_reply.started":"2024-11-13T09:55:28.212652Z","shell.execute_reply":"2024-11-13T09:55:28.549835Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"- **Model Loading**: With bnb_config, weights are likely loaded in 4-bit precision if use_4bit is True, saving memory.\n- **Training**: If fp16=True, training occurs in 16-bit precision (float16). If bf16=True and the GPU supports it, bfloat16 is used instead.","metadata":{}},{"cell_type":"markdown","source":"### Passing all the defined arguments toe corresponed fucntions to perform finetuning Lama on specific label data","metadata":{}},{"cell_type":"code","source":"# working on\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\") # Loads only the training portion of the dataset.\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\n#  This block configures model quantization to reduce memory usage by using 4-bit quantization for model weights.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16 \nif compute_dtype == torch.float16 and use_4bit:  # This section checks if the GPU has bfloat16 capability, which can further improve performance and stability if available. If compatible, you might switch compute_dtype to bfloat16 for training.\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n#  Load Base Model with Quantization  This loads the base model as a causal language model with specific quantization parameters and configurations for training\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False  # Disables caching to save memory.\nmodel.config.pretraining_tp = 1 # Ensures the model is ready for training in the pretraining pipeline format.\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token  # Sets the pad_token to eos_token to mark the end of sequences.\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\", #Indicates if any bias should be added; none means no additional bias.\n    task_type=\"CAUSAL_LM\", # Sets the type of task, which here is CAUSAL_LM for causal language modeling\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-13T09:55:31.751309Z","iopub.execute_input":"2024-11-13T09:55:31.752073Z","iopub.status.idle":"2024-11-13T10:01:26.416410Z","shell.execute_reply.started":"2024-11-13T09:55:31.752031Z","shell.execute_reply":"2024-11-13T10:01:26.415445Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdbf4673631849e4b8ae12e8d0d585fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1031b03fab5a4e0ead75aeb30c446119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7667e86fa149fbaa8cf821ae1506ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ffccab398f45eabbbceba678522ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9e2574a332d46e4a04d6cd97ad249b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08061cee039246048a6ba64bbde19815"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1fd564e838436791f5fe61c9cae6bf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 04:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.023100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.761100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.840800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.909700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.725200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.786300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.781200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.775100</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.774800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.757100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=1.8134216918945312, metrics={'train_runtime': 287.2492, 'train_samples_per_second': 3.481, 'train_steps_per_second': 0.87, 'total_flos': 1257334296109056.0, 'train_loss': 1.8134216918945312, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving the fine tuned model on specific data","metadata":{}},{"cell_type":"code","source":"# working on\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:01:26.418314Z","iopub.execute_input":"2024-11-13T10:01:26.419017Z","iopub.status.idle":"2024-11-13T10:01:26.644651Z","shell.execute_reply.started":"2024-11-13T10:01:26.418969Z","shell.execute_reply":"2024-11-13T10:01:26.643603Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# working on\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:01:26.646113Z","iopub.execute_input":"2024-11-13T10:01:26.646525Z","iopub.status.idle":"2024-11-13T10:02:13.088437Z","shell.execute_reply.started":"2024-11-13T10:01:26.646471Z","shell.execute_reply":"2024-11-13T10:02:13.087439Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<s>[INST] What is a large language model? [/INST] A large language model is a deep learning model that is trained on a large amount of text data. It is trained to generate text that is similar to the text it is trained on. Large language models are used in a variety of applications, including natural language processing, machine translation, and question answering. [/s]\n<s>[INST] How do large language models work? [/INST] Large language models are trained on a large amount of text data, and they are trained to generate text that is similar to the text they are trained on. They are trained in a way that they can learn from the data they are trained on, and they can generate text that is similar to the text they are trained on. [/s]\n<s>[INST] What are some examples of large language models? [/INST] Some examples of large language models include GPT-3, BERT, and OpenAI’s GPT-2.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}